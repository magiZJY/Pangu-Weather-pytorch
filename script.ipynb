{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "-BQ5FK9aLfzR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def Pad3D(x, padding=(1, 1, 1, 1, 1, 1)):\n",
    "    \"\"\"\n",
    "    Apply 3D padding to a 5D tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - x: A 5D tensor of shape [batch_size, channels, depth, height, width].\n",
    "    - padding: A tuple of 6 integers (padLeft, padRight, padTop, padBottom, padFront, padBack)\n",
    "               specifying the amount of padding to add.\n",
    "\n",
    "    Returns:\n",
    "    - A 5D tensor with padding applied.\n",
    "    \"\"\"\n",
    "    print(x.shape)\n",
    "    return F.pad(x, padding, mode='constant', value=0)\n",
    "\n",
    "\n",
    "def Pad2D(x, padding=(1, 1, 1, 1)):\n",
    "    \"\"\"\n",
    "    Apply 2D padding to a 4D tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - x: A 4D tensor of shape [batch_size, channels, height, width].\n",
    "    - padding: A tuple of 4 integers (padLeft, padRight, padTop, padBottom)\n",
    "               specifying the amount of padding to add.\n",
    "\n",
    "    Returns:\n",
    "    - A 4D tensor with padding applied.\n",
    "    \"\"\"\n",
    "    return F.pad(x, padding, mode='constant', value=0)\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(1, 1, 10, 10)  # Example tensor\n",
    "padding = (1, 1, 1, 1)  # Add a padding of 1 unit around each side in the height and width dimensions\n",
    "\n",
    "def gen_mask(x):\n",
    "\n",
    "    return\n",
    "\n",
    "def no_mask():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47k4Z2rR7bHY"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gopCWiZU5MLg",
    "outputId": "65138d25-dc03-493b-828e-af41941165c0"
   },
   "outputs": [],
   "source": [
    "# pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgOJAW_06K4w",
    "outputId": "98c22e17-c99f-4912-ac0e-18ad0a6766ec"
   },
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/pvigier/perlin-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z175vEkC7d_9"
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Rd8rUXVp3bKA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import reshape, permute, stack, flatten\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import GELU, Dropout, LayerNorm, Softmax, Linear, Conv2d, Conv3d, ConvTranspose2d, ConvTranspose3d\n",
    "from timm.models.layers import DropPath\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "\n",
    "from perlin_numpy import generate_fractal_noise_3d\n",
    "\n",
    "# from helper import roll3D, Pad3D, Pad2D, Crop3D, Crop2D, gen_mask, no_mask\n",
    "\n",
    "# Common functions for creating new tensors\n",
    "# ConstructTensor: create a new tensor with an arbitrary shape\n",
    "# TruncatedNormalInit: Initialize the tensor with Truncate Normalization distribution\n",
    "# RangeTensor: create a new tensor like range(a, b)\n",
    "# from Your_AI_Library import ConstructTensor, TruncatedNormalInit\n",
    "ConstructTensor = torch.ones\n",
    "RangeTensor = torch.range\n",
    "TruncatedNormalInit = torch.nn.init.normal_\n",
    "# Custom functions to read your data from the disc\n",
    "# LoadData: Load the ERA5 data\n",
    "# LoadConstantMask: Load constant masks, e.g., soil type\n",
    "# LoadStatic: Load mean and std of the ERA5 training data, every fields such as T850 is treated as an image and calculate the mean and std\n",
    "# from Your_Data_Code import LoadData, LoadConstantMask, LoadStatic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "0u-yAt1-Lt8G"
   },
   "outputs": [],
   "source": [
    "# dummy load data\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, size=100, channels=4, depth=4, height=358, width=179):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (int): Number of data points.\n",
    "            channels (int): Channel dimension of the data.\n",
    "            depth (int): Depth dimension of the data.\n",
    "            height (int): Height dimension of the data.\n",
    "            width (int): Width dimension of the data.\n",
    "        \"\"\"\n",
    "        # Ensure the product of depth, height, and width matches 521280\n",
    "        self.inputs = torch.randn(size, channels, depth, height, width)  # Shape: (size, 4, 8, 360, 181)\n",
    "        self.inputs_surface = torch.randn(size, channels, 2, height, width)  # Same shape as inputs\n",
    "        self.targets = torch.randn(size, channels, depth, height, width)  # Assuming target shape is same as input\n",
    "        self.targets_surface = torch.randn(size, channels, 2, height, width)  # Assuming target shape is same as input surface\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.inputs_surface[idx], self.targets[idx], self.targets_surface[idx]\n",
    "\n",
    "\n",
    "        \n",
    "def LoadData(batch_size=32, shuffle=True):\n",
    "    dataset = DummyDataset()\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "def LoadConstantMask():\n",
    "    return torch.rand((721, 1440)), torch.rand((721, 1440)), torch.rand((721, 1440))\n",
    "def LoadStatic():\n",
    "    return torch.rand((721, 1440)), torch.rand((721, 1440)), torch.rand((721, 1440))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1vZBE3l7g7w"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FNcbCx3qNvzr"
   },
   "outputs": [],
   "source": [
    "# hi = LoadData()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXYfuOcttpSl"
   },
   "source": [
    "# ClassDefinitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXYfuOcttpSl"
   },
   "source": [
    "## PanguModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "K65MmSehtFok"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PanguModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    # Drop path rate is linearly increased as the depth increases\n",
    "    super().__init__()\n",
    "    drop_path_list = torch.linspace(0, 0.2, 8)\n",
    "\n",
    "    # Patch embedding\n",
    "    self._input_layer = PatchEmbedding((2, 4, 4), 192)\n",
    "\n",
    "    # Four basic layers\n",
    "    self.layer1 = EarthSpecificLayer(2, 192, drop_path_list[:2], 6)\n",
    "    self.layer2 = EarthSpecificLayer(6, 384, drop_path_list[2:], 12)\n",
    "    self.layer3 = EarthSpecificLayer(6, 384, drop_path_list[2:], 12)\n",
    "    self.layer4 = EarthSpecificLayer(2, 192, drop_path_list[:2], 6)\n",
    "\n",
    "    # Upsample and downsample\n",
    "    self.upsample = UpSample(384, 192)\n",
    "    self.downsample = DownSample(192)\n",
    "\n",
    "    # Patch Recovery\n",
    "    self._output_layer = PatchRecovery((2,4,4), 384) # TODO: dummy patch\n",
    "\n",
    "  def forward(self, input, input_surface):\n",
    "    '''Backbone architecture'''\n",
    "    # Embed the input fields into patches\n",
    "    x = self._input_layer(input, input_surface)\n",
    "\n",
    "    # Encoder, composed of two layers\n",
    "    # Layer 1, shape (8, 360, 181, C), C = 192 as in the original paper\n",
    "    x = self.layer1(x, 8, 360, 181)\n",
    "\n",
    "    # Store the tensor for skip-connection\n",
    "    skip = x\n",
    "\n",
    "    # Downsample from (8, 360, 181) to (8, 180, 91)\n",
    "    x = self.downsample(x, 8, 360, 181)\n",
    "\n",
    "    # Layer 2, shape (8, 180, 91, 2C), C = 192 as in the original paper\n",
    "    x = self.layer2(x, 8, 180, 91)\n",
    "\n",
    "    # Decoder, composed of two layers\n",
    "    # Layer 3, shape (8, 180, 91, 2C), C = 192 as in the original paper\n",
    "    x = self.layer3(x, 8, 180, 91)\n",
    "\n",
    "    # Upsample from (8, 180, 91) to (8, 360, 181)\n",
    "    x = self.upsample(x)\n",
    "\n",
    "    # Layer 4, shape (8, 360, 181, 2C), C = 192 as in the original paper\n",
    "    x = self.layer4(x, 8, 360, 181)\n",
    "\n",
    "    # Skip connect, in last dimension(C from 192 to 384)\n",
    "    x = torch.cat(skip, x, dim=-1)\n",
    "\n",
    "    # Recover the output fields from patches\n",
    "    output, output_surface = self._output_layer(x)\n",
    "    return output, output_surface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXYfuOcttpSl"
   },
   "source": [
    "## PatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8rq2JZN8tKMl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, patch_size, dim):\n",
    "    '''Patch embedding operation'''\n",
    "    super().__init__()\n",
    "    # Here we use convolution to partition data into cubes\n",
    "    self.conv = Conv2d(in_channels=5, out_channels=dim, kernel_size=patch_size, stride=patch_size)\n",
    "    self.conv_surface = Conv2d(in_channels=7, out_channels=dim, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "\n",
    "    # Load constant masks from the disc\n",
    "    self.land_mask, self.soil_type, self.topography = LoadConstantMask()\n",
    "\n",
    "  def forward(self, input, input_surface):\n",
    "    # Zero-pad the input ??? remove and try to see shape?\n",
    "    print(input.shape)\n",
    "    print(input_surface.shape)\n",
    "    input = Pad3D(input)\n",
    "    print(\"input shape after 3D padding: \", input.shape)\n",
    "    input_surface = Pad2D(input_surface)\n",
    "    print(\"input surface shape 2D after padding: \", input_surface.shape)\n",
    "    # Apply a linear projection for patch_size[0]*patch_size[1]*patch_size[2] patches, patch_size = (2, 4, 4) as in the original paper\n",
    "    # input = self.conv(input)\n",
    "\n",
    "    # Add three constant fields to the surface fields\n",
    "    # input_surface = torch.concatenate((input_surface, self.land_mask, self.soil_type, self.topography))\n",
    "\n",
    "    # Apply a linear projection for patch_size[1]*patch_size[2] patches\n",
    "    # input_surface = self.conv_surface(input_surface)\n",
    "\n",
    "    # Concatenate the input in the pressure level, i.e., in Z dimension\n",
    "    x = torch.concat((input, input_surface), dim=2)\n",
    "    print(x.shape)\n",
    "    # Reshape x for calculation of linear projections\n",
    "    x = torch.permute(x, (0, 2, 3, 4, 1))\n",
    "    x = reshape(x, shape=(x.shape[0], 8*360*181, x.shape[-1]))\n",
    "    print(\"dummy x successfully constructed with shape:\", x.shape)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oyNDEKSWtM8y"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchRecovery(nn.Module):\n",
    "  def __init__(self, patch_size, dim):\n",
    "    super().__init__()\n",
    "    '''Patch recovery operation'''\n",
    "    # Hear we use two transposed convolutions to recover data\n",
    "    self.conv = ConvTranspose3d(in_channels=dim, out_channels=5, kernel_size=patch_size, stride=patch_size)\n",
    "    self.conv_surface = ConvTranspose2d(in_channels=dim, out_channels=4, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "\n",
    "  def forward(self, x, Z, H, W):\n",
    "    # The inverse operation of the patch embedding operation, patch_size = (2, 4, 4) as in the original paper\n",
    "    # Reshape x back to three dimensions\n",
    "    x = permute(x, (0, 2, 1))\n",
    "    x = reshape(x, shape=(x.shape[0], x.shape[1], Z, H, W))\n",
    "\n",
    "    # Call the transposed convolution\n",
    "    output = self.conv(x[:, :, 1:, :, :])\n",
    "    output_surface = self.conv_surface(x[:, :, 0, :, :])\n",
    "\n",
    "    # Crop the output to remove zero-paddings\n",
    "    output = Crop3D(output)\n",
    "    output_surface = Crop2D(output_surface)\n",
    "    return output, output_surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vRJTgvZltRzl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DownSample(nn.Module):\n",
    "  def __init__(self, dim):\n",
    "    super().__init__()\n",
    "    '''Down-sampling operation'''\n",
    "    # A linear function and a layer normalization\n",
    "    self.linear = Linear(4*dim, 2*dim, bias=False)\n",
    "    self.norm = LayerNorm(4*dim)\n",
    "\n",
    "  def forward(self, x, Z, H, W):\n",
    "    # Reshape x to three dimensions for downsampling\n",
    "    x = reshape(x, shape=(x.shape[0], Z, H, W, x.shape[-1]))\n",
    "\n",
    "    # Padding the input to facilitate downsampling\n",
    "    x = Pad3D(x)\n",
    "\n",
    "    # Reorganize x to reduce the resolution: simply change the order and downsample from (8, 360, 182) to (8, 180, 91)\n",
    "    Z, H, W = x.shape\n",
    "    # Reshape x to facilitate downsampling\n",
    "    x = reshape(x, shape=(x.shape[0], Z, H//2, 2, W//2, 2, x.shape[-1]))\n",
    "    # Change the order of x\n",
    "    x = permute(x, (0,1,2,4,3,5,6))\n",
    "    # Reshape to get a tensor of resolution (8, 180, 91)\n",
    "    x = reshape(x, shape=(x.shape[0], Z*(H//2)*(W//2), 4 * x.shape[-1]))\n",
    "\n",
    "    # Call the layer normalization\n",
    "    x = self.norm(x)\n",
    "\n",
    "    # Decrease the channels of the data to reduce computation cost\n",
    "    x = self.linear(x)\n",
    "    return x\n",
    "\n",
    "class UpSample:\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    '''Up-sampling operation'''\n",
    "    \n",
    "    # Linear layers without bias to increase channels of the data\n",
    "    self.linear1 = Linear(input_dim, output_dim*4, bias=False)\n",
    "\n",
    "    # Linear layers without bias to mix the data up\n",
    "    self.linear2 = Linear(output_dim, output_dim, bias=False)\n",
    "\n",
    "    # Normalization\n",
    "    self.norm = LayerNorm(output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Call the linear functions to increase channels of the data\n",
    "    x = self.linear1(x)\n",
    "\n",
    "    # Reorganize x to increase the resolution: simply change the order and upsample from (8, 180, 91) to (8, 360, 182)\n",
    "    # Reshape x to facilitate upsampling.\n",
    "    x = reshape(x, shape=(x.shape[0], 8, 180, 91, 2, 2, x.shape[-1]//4))\n",
    "    # Change the order of x\n",
    "    x = permute(x, (0,1,2,4,3,5,6))\n",
    "    # Reshape to get Tensor with a resolution of (8, 360, 182)\n",
    "    x = reshape(x, shape=(x.shape[0], 8, 360, 182, x.shape[-1]))\n",
    "\n",
    "    # Crop the output to the input shape of the network\n",
    "    x = Crop3D(x)\n",
    "\n",
    "    # Reshape x back\n",
    "    x = reshape(x, shape=(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[-1]))\n",
    "\n",
    "    # Call the layer normalization\n",
    "    x = self.norm(x)\n",
    "\n",
    "    # Mixup normalized tensors\n",
    "    x = self.linear2(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "9lf03TyftVg-"
   },
   "outputs": [],
   "source": [
    "\n",
    "class EarthSpecificLayer(nn.Module):\n",
    "  def __init__(self, depth, dim, drop_path_ratio_list, heads):\n",
    "    '''Basic layer of our network, contains 2 or 6 blocks'''\n",
    "    super().__init__()\n",
    "    self.depth = depth\n",
    "    self.blocks = []\n",
    "\n",
    "    # Construct basic blocks\n",
    "    print(dim, \"\\ndrop_path_ratio_list: \", drop_path_ratio_list.shape, \"\\nheads: \",heads)\n",
    "    for i in range(depth):\n",
    "      # print()\n",
    "      self.blocks.append(EarthSpecificBlock(dim, drop_path_ratio_list[i], heads))\n",
    "      # self.blocks.append(EarthSpecificBlock(dim, heads))\n",
    "\n",
    "  def forward(self, x, Z, H, W):\n",
    "    for i in range(self.depth):\n",
    "      # Roll the input every two blocks\n",
    "      if i % 2 == 0:\n",
    "        self.blocks[i](x, Z, H, W, roll=False)\n",
    "      else:\n",
    "        self.blocks[i](x, Z, H, W, roll=True)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CUMpivY8tY4v"
   },
   "outputs": [],
   "source": [
    "class EarthSpecificBlock(nn.Module):\n",
    "  def __init__(self, dim, drop_path_ratio, heads):\n",
    "    super().__init__()\n",
    "    '''\n",
    "    3D transformer block with Earth-Specific bias and window attention,\n",
    "    see https://github.com/microsoft/Swin-Transformer for the official implementation of 2D window attention.\n",
    "    The major difference is that we expand the dimensions to 3 and replace the relative position bias with Earth-Specific bias.\n",
    "    '''\n",
    "    # Define the window size of the neural network\n",
    "    self.window_size = (2, 6, 12)\n",
    "\n",
    "    # Initialize serveral operations\n",
    "    self.drop_path = DropPath(drop_prob=drop_path_ratio)\n",
    "    self.norm1 = LayerNorm(dim)\n",
    "    self.norm2 = LayerNorm(dim)\n",
    "    self.linear = MLP(dim, 0)\n",
    "    self.attention = EarthAttention3D(dim, heads, 0, self.window_size)\n",
    "\n",
    "  def forward(self, x, Z, H, W, roll):\n",
    "    # Save the shortcut for skip-connection\n",
    "    shortcut = x\n",
    "\n",
    "    # Reshape input to three dimensions to calculate window attention\n",
    "    x = reshape(x, shape=(x.shape[0], Z, H, W, x.shape[2]))\n",
    "\n",
    "    # Zero-pad input if needed\n",
    "    x = Pad3D(x)\n",
    "\n",
    "    # Store the shape of the input for restoration\n",
    "    ori_shape = x.shape\n",
    "\n",
    "    if roll:\n",
    "      # Roll x for half of the window for 3 dimensions\n",
    "      x = roll3D(x, shift=[self.window_size[0]//2, self.window_size[1]//2, self.window_size[2]//2])\n",
    "      # Generate mask of attention masks\n",
    "      # If two pixels are not adjacent, then mask the attention between them\n",
    "      # Your can set the matrix element to -1000 when it is not adjacent, then add it to the attention\n",
    "      mask = gen_mask(x)\n",
    "    else:\n",
    "      # e.g., zero matrix when you add mask to attention\n",
    "      mask = no_mask()\n",
    "\n",
    "    # Reorganize data to calculate window attention\n",
    "    x_window = reshape(x, shape=(x.shape[0], Z//self.window_size[0], self.window_size[0], H // self.window_size[1], self.window_size[1], W // self.window_size[2], self.window_size[2], x.shape[-1]))\n",
    "    x_window = permute(x_window, (0, 1, 3, 5, 2, 4, 6, 7))\n",
    "\n",
    "    # Get data stacked in 3D cubes, which will further be used to calculated attention among each cube\n",
    "    x_window = reshape(x_window, shape=(-1, self.window_size[0]* self.window_size[1]*self.window_size[2], x.shape[-1]))\n",
    "\n",
    "    # Apply 3D window attention with Earth-Specific bias\n",
    "    x_window = self.attention(x, mask)\n",
    "\n",
    "    # Reorganize data to original shapes\n",
    "    x = reshape(x_window, shape=((-1, Z // self.window_size[0], H // self.window_size[1], W // self.window_size[2], self.window_size[0], self.window_size[1], self.window_size[2], x_window.shape[-1])))\n",
    "    x = permute(x, (0, 1, 4, 2, 5, 3, 6, 7))\n",
    "\n",
    "    # Reshape the tensor back to its original shape\n",
    "    x = reshape(x_window, shape=ori_shape)\n",
    "\n",
    "    if roll:\n",
    "      # Roll x back for half of the window\n",
    "      x = roll3D(x, shift=[-self.window_size[0]//2, -self.window_size[1]//2, -self.window_size[2]//2])\n",
    "\n",
    "    # Crop the zero-padding\n",
    "    x = Crop3D(x)\n",
    "\n",
    "    # Reshape the tensor back to the input shape\n",
    "    x = reshape(x, shape=(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[4]))\n",
    "\n",
    "    # Main calculation stages\n",
    "    x = shortcut + self.drop_path(self.norm1(x))\n",
    "    x = x + self.drop_path(self.norm2(self.linear(x)))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "O3i-9G5BtZJF"
   },
   "outputs": [],
   "source": [
    "class EarthAttention3D(nn.Module): # TODO\n",
    "  def __init__(self, dim, heads, dropout_rate, window_size):\n",
    "    super().__init__()\n",
    "    '''\n",
    "    3D window attention with the Earth-Specific bias,\n",
    "    see https://github.com/microsoft/Swin-Transformer for the official implementation of 2D window attention.\n",
    "    '''\n",
    "    # Initialize several operations\n",
    "    self.linear1 = Linear(dim, 3, bias=True)\n",
    "    self.linear2 = Linear(dim, dim)\n",
    "    self.softmax = Softmax(dim=-1)\n",
    "    self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    # Store several attributes\n",
    "    self.head_number = heads\n",
    "    self.dim = dim\n",
    "    self.scale = (dim//heads)**-0.5\n",
    "    self.window_size = window_size\n",
    "\n",
    "    # input_shape is current shape of the self.forward function\n",
    "    # You can run your code to record it, modify the code and rerun it\n",
    "    # Record the number of different window types\n",
    "    # ?? not sure\n",
    "    self.type_of_windows = 10\n",
    "    # self.type_of_windows = (input_shape[0]//window_size[0])*(input_shape[1]//window_size[1])\n",
    "\n",
    "    # For each type of window, we will construct a set of parameters according to the paper\n",
    "    self.earth_specific_bias = ConstructTensor(size=((2 * window_size[2] - 1) * window_size[1] * window_size[1] * window_size[0] * window_size[0], self.type_of_windows, heads))\n",
    "\n",
    "    # Making these tensors to be learnable parameters\n",
    "    self.earth_specific_bias = nn.Parameter(self.earth_specific_bias)\n",
    "\n",
    "    # Initialize the tensors using Truncated normal distribution\n",
    "    TruncatedNormalInit(self.earth_specific_bias, std=0.02)\n",
    "\n",
    "    # Construct position index to reuse self.earth_specific_bias\n",
    "    self.position_index = self._construct_index()\n",
    "\n",
    "  def _construct_index(self):\n",
    "    ''' This function construct the position index to reuse symmetrical parameters of the position bias'''\n",
    "    # Index in the pressure level of query matrix\n",
    "    coords_zi = torch.arange(self.window_size[0])\n",
    "    # Index in the pressure level of key matrix\n",
    "    coords_zj = -torch.arange(self.window_size[0])*self.window_size[0]\n",
    "\n",
    "    # Index in the latitude of query matrix\n",
    "    coords_hi = torch.arange(self.window_size[1])\n",
    "    # Index in the latitude of key matrix\n",
    "    coords_hj = -torch.arange(self.window_size[1])*self.window_size[1]\n",
    "\n",
    "    # Index in the longitude of the key-value pair\n",
    "    coords_w = torch.arange(self.window_size[2])\n",
    "\n",
    "    # Change the order of the index to calculate the index in total\n",
    "    coords_1 = stack(torch.meshgrid([coords_zi, coords_hi, coords_w]))\n",
    "    coords_2 = stack(torch.meshgrid([coords_zj, coords_hj, coords_w]))\n",
    "    coords_flatten_1 = flatten(coords_1, start_dim=1)\n",
    "    coords_flatten_2 = flatten(coords_2, start_dim=1)\n",
    "    coords = coords_flatten_1[:, :, None] - coords_flatten_2[:, None, :]\n",
    "    coords = permute(coords, (1, 2, 0))\n",
    "\n",
    "    # Shift the index for each dimension to start from 0\n",
    "    coords[:, :, 2] += self.window_size[2] - 1\n",
    "    coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "    coords[:, :, 0] *= (2 * self.window_size[2] - 1)*self.window_size[1]*self.window_size[1]\n",
    "\n",
    "    # Sum up the indexes in three dimensions\n",
    "    self.position_index = torch.sum(coords, dim=-1)\n",
    "\n",
    "    # Flatten the position index to facilitate further indexing\n",
    "    self.position_index = flatten(self.position_index)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # Linear layer to create query, key and value\n",
    "    x = self.linear1(x)\n",
    "\n",
    "    # Record the original shape of the input\n",
    "    original_shape = x.shape\n",
    "\n",
    "    # reshape the data to calculate multi-head attention\n",
    "    qkv = reshape(x, shape=(x.shape[0], x.shape[1], 3, self.head_number, self.dim // self.head_number))\n",
    "    query, key, value = permute(qkv, (2, 0, 3, 1, 4))\n",
    "\n",
    "    # Scale the attention\n",
    "    query = query * self.scale\n",
    "\n",
    "    # Calculated the attention, a learnable bias is added to fix the nonuniformity of the grid.\n",
    "    attention = query @ key.T # @ denotes matrix multiplication\n",
    "\n",
    "    # self.earth_specific_bias is a set of neural network parameters to optimize.\n",
    "    EarthSpecificBias = self.earth_specific_bias[self.position_index]\n",
    "\n",
    "    # Reshape the learnable bias to the same shape as the attention matrix\n",
    "    EarthSpecificBias = reshape(EarthSpecificBias, shape=(self.window_size[0]*self.window_size[1]*self.window_size[2], self.window_size[0]*self.window_size[1]*self.window_size[2], self.type_of_windows, self.head_number))\n",
    "    EarthSpecificBias = permute(EarthSpecificBias, (2, 3, 0, 1))\n",
    "    EarthSpecificBias = reshape(EarthSpecificBias, shape = [1]+EarthSpecificBias.shape)\n",
    "\n",
    "    # Add the Earth-Specific bias to the attention matrix\n",
    "    attention = attention + EarthSpecificBias\n",
    "\n",
    "    # Mask the attention between non-adjacent pixels, e.g., simply add -100 to the masked element.\n",
    "    attention = self.mask_attention(attention, mask)\n",
    "    attention = self.softmax(attention)\n",
    "    attention = self.dropout(attention)\n",
    "\n",
    "    # Calculated the tensor after spatial mixing.\n",
    "    x = attention @ value.T # @ denote matrix multiplication\n",
    "\n",
    "    # Reshape tensor to the original shape\n",
    "    x = permute(x, (0, 2, 1))\n",
    "    x = reshape(x, shape = original_shape)\n",
    "\n",
    "    # Linear layer to post-process operated tensor\n",
    "    x = self.linear2(x)\n",
    "    x = self.dropout(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "irZt6Qjctc4M"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, dim, dropout_rate):\n",
    "    super().__init__()\n",
    "    '''MLP layers, same as most vision transformer architectures.'''\n",
    "    self.linear1 = Linear(dim, dim * 4)\n",
    "    self.linear2 = Linear(dim * 4, dim)\n",
    "    self.activation = GELU()\n",
    "    self.drop = Dropout(dropout_rate)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.linear(x)\n",
    "    x = self.drop(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iTXzOSV7LPEl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def Inference(input, input_surface, forecast_range):\n",
    "  '''Inference code, describing the algorithm of inference using models with different lead times.\n",
    "  PanguModel24, PanguModel6, PanguModel3 and PanguModel1 share the same training algorithm but differ in lead times.\n",
    "  Args:\n",
    "    input: input tensor, need to be normalized to N(0, 1) in practice\n",
    "    input_surface: target tensor, need to be normalized to N(0, 1) in practice\n",
    "    forecast_range: iteration numbers when roll out the forecast model\n",
    "  '''\n",
    "\n",
    "  PanguModel24 = LoadModel(ModelPath24)\n",
    "  PanguModel6 = LoadModel(ModelPath6)\n",
    "  PanguModel3 = LoadModel(ModelPath3)\n",
    "  PanguModel1 = LoadModel(ModelPath1)\n",
    "\n",
    "  weather_mean, weather_std, weather_surface_mean, weather_surface_std = LoadStatic()\n",
    "\n",
    "  input_24, input_surface_24 = input, input_surface\n",
    "  input_6, input_surface_6 = input, input_surface\n",
    "  input_3, input_surface_3 = input, input_surface\n",
    "\n",
    "  output_list = []\n",
    "\n",
    "  # Note: the following code is implemented for fast inference of [1,forecast_range]-hour forecasts -- if only one lead time is requested, the inference can be much faster.\n",
    "  for i in range(forecast_range):\n",
    "    # switch to the 24-hour model if the forecast time is 24 hours, 48 hours, ..., 24*N hours\n",
    "    if (i+1) % 24 == 0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_24, input_surface_24\n",
    "\n",
    "      # Call the model pretrained for 24 hours forecast\n",
    "      output, output_surface = PanguModel24(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "      # Stored the output for next round forecast\n",
    "      input_24, input_surface_24 = output, output_surface\n",
    "      input_6, input_surface_6 = output, output_surface\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 6-hour model if the forecast time is 30 hours, 36 hours, ..., 24*N + 6/12/18 hours\n",
    "    elif (i+1) % 6 == 0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_6, input_surface_6\n",
    "\n",
    "      # Call the model pretrained for 6 hours forecast\n",
    "      output, output_surface = PanguModel6(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "      # Stored the output for next round forecast\n",
    "      input_6, input_surface_6 = output, output_surface\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 3-hour model if the forecast time is 3 hours, 9 hours, ..., 6*N + 3 hours\n",
    "    elif (i+1) % 3 ==0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_3, input_surface_3\n",
    "\n",
    "      # Call the model pretrained for 3 hours forecast\n",
    "      output, output_surface = PanguModel3(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "      # Stored the output for next round forecast\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 1-hour model\n",
    "    else:\n",
    "      # Call the model pretrained for 1 hours forecast\n",
    "      output, output_surface = PanguModel1(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "    # Stored the output for next round forecast\n",
    "    input, input_surface = output, output_surface\n",
    "\n",
    "    # Save the output\n",
    "    output_list.append((output, output_surface))\n",
    "  return output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Z_ODpzJms_J7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    model = PanguModel()\n",
    "    optimizer = Adam(model.parameters(), lr=5e-4, weight_decay=3e-6)\n",
    "    epochs = 100\n",
    "    for i in range(epochs):  # Loop from 1979 to 2017\n",
    "        total_loss = 0\n",
    "        dataloader = LoadData(batch_size=32, shuffle=True)\n",
    "        for datastep in dataloader:\n",
    "            # Unpack the data\n",
    "            datastep\n",
    "            inputs, inputs_surface, targets, targets_surface = datastep\n",
    "            # inputs, inputs_surface, targets, targets_surface = inputs.to(device), inputs_surface.to(device), targets.to(device), targets_surface.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, outputs_surface = model(inputs, inputs_surface)\n",
    "\n",
    "            # Calculate loss: MAE loss for both output and surface output, with additional weight for the surface loss\n",
    "            # loss = TensorAbs(output-target) + TensorAbs(output_surface-target_surface) * 0.25\n",
    "            loss = F.l1_loss(outputs, targets) + 0.25 * F.l1_loss(outputs_surface, targets_surface)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{i+1}/{epochs}], Loss: {total_loss/len(dataloader)}')\n",
    "    torch.save(model.state_dict(), 'pangu_weather_model.pth')\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOWgXuIFt2Wy"
   },
   "source": [
    "## here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "R3LjkOGWuhHM"
   },
   "outputs": [],
   "source": [
    "## here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "KxHmH282uTdg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 \n",
      "drop_path_ratio_list:  torch.Size([2]) \n",
      "heads:  6\n",
      "384 \n",
      "drop_path_ratio_list:  torch.Size([6]) \n",
      "heads:  12\n",
      "384 \n",
      "drop_path_ratio_list:  torch.Size([6]) \n",
      "heads:  12\n",
      "192 \n",
      "drop_path_ratio_list:  torch.Size([2]) \n",
      "heads:  6\n",
      "torch.Size([32, 4, 4, 358, 179])\n",
      "torch.Size([32, 4, 2, 358, 179])\n",
      "torch.Size([32, 4, 4, 358, 179])\n",
      "input shape after 3D padding:  torch.Size([32, 4, 6, 360, 181])\n",
      "input surface shape 2D after padding:  torch.Size([32, 4, 2, 360, 181])\n",
      "torch.Size([32, 4, 8, 360, 181])\n",
      "dummy x successfully constructed with shape: torch.Size([32, 521280, 4])\n",
      "torch.Size([32, 8, 360, 181, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 4, 2, 60, 6, 15, 12, 6]' is invalid for input of size 101753856",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs, outputs_surface \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_surface\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate loss: MAE loss for both output and surface output, with additional weight for the surface loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# loss = TensorAbs(output-target) + TensorAbs(output_surface-target_surface) * 0.25\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39ml1_loss(outputs, targets) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m*\u001b[39m F\u001b[38;5;241m.\u001b[39ml1_loss(outputs_surface, targets_surface)\n",
      "File \u001b[0;32m~/test/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m, in \u001b[0;36mPanguModel.forward\u001b[0;34m(self, input, input_surface)\u001b[0m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_layer(\u001b[38;5;28minput\u001b[39m, input_surface)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Encoder, composed of two layers\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Layer 1, shape (8, 360, 181, C), C = 192 as in the original paper\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m360\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m181\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Store the tensor for skip-connection\u001b[39;00m\n\u001b[1;32m     33\u001b[0m skip \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/test/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[48], line 19\u001b[0m, in \u001b[0;36mEarthSpecificLayer.forward\u001b[0;34m(self, x, Z, H, W)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth):\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;66;03m# Roll the input every two blocks\u001b[39;00m\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[i](x, Z, H, W, roll\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/test/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m, in \u001b[0;36mEarthSpecificBlock.forward\u001b[0;34m(self, x, Z, H, W, roll)\u001b[0m\n\u001b[1;32m     41\u001b[0m   mask \u001b[38;5;241m=\u001b[39m no_mask()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Reorganize data to calculate window attention\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m x_window \u001b[38;5;241m=\u001b[39m \u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m x_window \u001b[38;5;241m=\u001b[39m permute(x_window, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Get data stacked in 3D cubes, which will further be used to calculated attention among each cube\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 4, 2, 60, 6, 15, 12, 6]' is invalid for input of size 101753856"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
