{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYlCtfKfLnPX",
    "outputId": "1ec171e0-8203-4bd7-923b-2e9db0f37758"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "181 %12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-BQ5FK9aLfzR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def Pad2D(data, patch_size=(4, 4)):\n",
    "    \"\"\"\n",
    "    Pad 2D surface input\n",
    "    \"\"\"\n",
    "    _, latitude, _  = data.shape\n",
    "\n",
    "    latitude_padding = (4 - latitude % 4) % 4\n",
    "    padded_data = F.pad(data, (0, 0, 0, latitude_padding), mode='constant', value=0)\n",
    "\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Pad3D(data, patch=(2,4,4)):\n",
    "    \"\"\"\n",
    "    Pad upper air var.\n",
    "    \"\"\"\n",
    "    try: # padding for patch emb\n",
    "      height, longitude, latitude, _ = data.shape # pad height(13) and lat(721)\n",
    "      # patch = (2,4,4)\n",
    "\n",
    "    except: # padding for earth spec block\n",
    "      _, height, longitude, latitude, _ = data.shape\n",
    "      # patch = (2, 6, 12) # window patch\n",
    "\n",
    "    height_patch = patch[0]\n",
    "    longitude_patch = patch[1]\n",
    "    latitude_patch = patch[2] # 181 pad 12\n",
    "\n",
    "\n",
    "    height_padding = (height_patch - height % height_patch) % height_patch\n",
    "    latitude_padding = (latitude_patch - latitude % latitude_patch) % latitude_patch\n",
    "    # print(\"latitude_padding\", latitude_padding)\n",
    "    # Apply padding\n",
    "    padded_data = F.pad(data, (0, 0, 0, latitude_padding, 0, 0, 0, height_padding), mode='constant', value=0)\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "\n",
    "\n",
    "def gen_mask(x):\n",
    "\n",
    "    return\n",
    "\n",
    "def no_mask():\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2O3Da5Vw-gw9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47k4Z2rR7bHY"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gopCWiZU5MLg",
    "outputId": "ca475472-d60b-4579-d015-5ebeb963a9b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch->timm)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 timm-0.9.16\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgOJAW_06K4w",
    "outputId": "ad98b5bb-3b59-49cd-b6c9-daf136d58e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/pvigier/perlin-numpy\n",
      "  Cloning https://github.com/pvigier/perlin-numpy to /tmp/pip-req-build-mswktpwh\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/pvigier/perlin-numpy /tmp/pip-req-build-mswktpwh\n",
      "  Resolved https://github.com/pvigier/perlin-numpy to commit 5e26837db14042e51166eb6cad4c0df2c1907016\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from perlin-numpy==0.0.0) (1.25.2)\n",
      "Building wheels for collected packages: perlin-numpy\n",
      "  Building wheel for perlin-numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for perlin-numpy: filename=perlin_numpy-0.0.0-py3-none-any.whl size=4718 sha256=4ac2beae971226f9b672ce4476405585ccc1c727bc443cbd21b458a61e997f8f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bx910ott/wheels/d0/67/1b/569e9d602b147a53e4ab67a3592944572d7d3886dc3a2e095e\n",
      "Successfully built perlin-numpy\n",
      "Installing collected packages: perlin-numpy\n",
      "Successfully installed perlin-numpy-0.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/pvigier/perlin-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z175vEkC7d_9"
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rd8rUXVp3bKA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import reshape, permute, stack, flatten\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import GELU, Dropout, LayerNorm, Softmax, Linear, Conv2d, Conv3d, ConvTranspose2d, ConvTranspose3d\n",
    "from timm.models.layers import DropPath\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "\n",
    "from perlin_numpy import generate_fractal_noise_3d\n",
    "\n",
    "# from helper import roll3D, Pad3D, Pad2D, Crop3D, Crop2D, gen_mask, no_mask\n",
    "\n",
    "# Common functions for creating new tensors\n",
    "# ConstructTensor: create a new tensor with an arbitrary shape\n",
    "# TruncatedNormalInit: Initialize the tensor with Truncate Normalization distribution\n",
    "# RangeTensor: create a new tensor like range(a, b)\n",
    "# from Your_AI_Library import ConstructTensor, TruncatedNormalInit\n",
    "ConstructTensor = torch.ones\n",
    "RangeTensor = torch.range\n",
    "TruncatedNormalInit = torch.nn.init.normal_\n",
    "roll3D = torch.roll\n",
    "# Custom functions to read your data from the disc\n",
    "# LoadData: Load the ERA5 data\n",
    "# LoadConstantMask: Load constant masks, e.g., soil type\n",
    "# LoadStatic: Load mean and std of the ERA5 training data, every fields such as T850 is treated as an image and calculate the mean and std\n",
    "# from Your_Data_Code import LoadData, LoadConstantMask, LoadStatic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0u-yAt1-Lt8G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ct1mUjRUjsLG"
   },
   "outputs": [],
   "source": [
    "# def LoadData(step):\n",
    "#     # dummy load for each step, we have upper air and surface input\n",
    "#     inputs = torch.randn(13, 1440, 721, 5)\n",
    "#     inputs_surface = torch.randn(1440, 721, 4)\n",
    "\n",
    "#     targets = torch.randn(4, 13, 1440, 721, 5)\n",
    "#     targets_surface = torch.randn(4, 1440, 721, 4)\n",
    "\n",
    "\n",
    "#     return inputs, inputs_surface, targets, targets_surface\n",
    "\n",
    "# def LoadConstantMask():\n",
    "#     return torch.rand((1440, 721)), torch.rand((1440, 721)), torch.rand((1440, 721))\n",
    "\n",
    "def LoadData(step):\n",
    "    # dummy load for each step, we have upper air and surface input\n",
    "    inputs = torch.ones((13, 1440, 721, 5))\n",
    "    inputs_surface = torch.ones((1440, 721, 4))\n",
    "\n",
    "    targets = torch.ones((4, 13, 1440, 721, 5))\n",
    "    targets_surface = torch.ones((4, 1440, 721, 4))\n",
    "\n",
    "\n",
    "    return inputs, inputs_surface, targets, targets_surface\n",
    "\n",
    "def LoadConstantMask():\n",
    "    return torch.ones((1440, 721)), torch.ones((1440, 721)), torch.ones((1440, 721))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1vZBE3l7g7w"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FNcbCx3qNvzr"
   },
   "outputs": [],
   "source": [
    "# hi = LoadData()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXYfuOcttpSl"
   },
   "source": [
    "# ClassDefinitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-2p6iURjYHb"
   },
   "source": [
    "## PanguModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K65MmSehtFok"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PanguModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    # Drop path rate is linearly increased as the depth increases\n",
    "    super().__init__()\n",
    "    drop_path_list = torch.linspace(0, 0.2, 8)\n",
    "\n",
    "    # Patch embedding\n",
    "    self._input_layer = PatchEmbedding((2, 4, 4), 192)\n",
    "\n",
    "    # Four basic layers\n",
    "    self.layer1 = EarthSpecificLayer(2, 192, drop_path_list[:2], 6)\n",
    "    self.layer2 = EarthSpecificLayer(6, 384, drop_path_list[2:], 12)\n",
    "    self.layer3 = EarthSpecificLayer(6, 384, drop_path_list[2:], 12)\n",
    "    self.layer4 = EarthSpecificLayer(2, 192, drop_path_list[:2], 6)\n",
    "\n",
    "    # Upsample and downsample\n",
    "    self.upsample = UpSample(384, 192)\n",
    "    self.downsample = DownSample(192)\n",
    "\n",
    "    # Patch Recovery\n",
    "    self._output_layer = PatchRecovery((2,4,4), 384) # TODO: dummy patch\n",
    "\n",
    "  def forward(self, input, input_surface):\n",
    "    '''Backbone architecture'''\n",
    "    # Embed the input fields into patches\n",
    "    x = self._input_layer(input, input_surface)\n",
    "    # print(\"Embededed the input fields into patches:\", x.shape)\n",
    "\n",
    "    # Encoder, composed of two layers\n",
    "    # Layer 1, shape (8, 360, 181, C), C = 192 as in the original paper\n",
    "    x = self.layer1(x, 8, 360, 181)\n",
    "\n",
    "    # Store the tensor for skip-connection\n",
    "    skip = x\n",
    "\n",
    "    # Downsample from (8, 360, 181) to (8, 180, 91)\n",
    "    x = self.downsample(x, 8, 360, 181)\n",
    "\n",
    "    # Layer 2, shape (8, 180, 91, 2C), C = 192 as in the original paper\n",
    "    x = self.layer2(x, 8, 180, 91)\n",
    "\n",
    "    # Decoder, composed of two layers\n",
    "    # Layer 3, shape (8, 180, 91, 2C), C = 192 as in the original paper\n",
    "    x = self.layer3(x, 8, 180, 91)\n",
    "\n",
    "    # Upsample from (8, 180, 91) to (8, 360, 181)\n",
    "    x = self.upsample(x)\n",
    "\n",
    "    # Layer 4, shape (8, 360, 181, 2C), C = 192 as in the original paper\n",
    "    x = self.layer4(x, 8, 360, 181)\n",
    "\n",
    "    # Skip connect, in last dimension(C from 192 to 384)\n",
    "    x = torch.cat(skip, x, dim=-1)\n",
    "\n",
    "    # Recover the output fields from patches\n",
    "    output, output_surface = self._output_layer(x)\n",
    "    return output, output_surface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O8AcV81jYHb"
   },
   "source": [
    "## PatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8rq2JZN8tKMl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, patch_size, dim):\n",
    "    '''Patch embedding operation'''\n",
    "    # called with patch_size = (2,4,4), dim=192\n",
    "    super().__init__()\n",
    "    # Here we use convolution to partition data into cubes\n",
    "    # self.conv = Conv2d(in_channels=5, out_channels=dim, kernel_size=patch_size, stride=patch_size)\n",
    "    # self.conv_surface = Conv2d(in_channels=7, out_channels=dim, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "\n",
    "    self.conv = Conv3d(in_channels=5, out_channels=dim, kernel_size=(2,4,4), stride=patch_size)\n",
    "    self.conv_surface = Conv2d(in_channels=4, out_channels=dim, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "\n",
    "\n",
    "    # Load constant masks from the disc\n",
    "    self.land_mask, self.soil_type, self.topography = LoadConstantMask()\n",
    "\n",
    "  def forward(self, input, input_surface):\n",
    "    # Zero-pad the input remove and try to see shape?\n",
    "    # print(input.shape)\n",
    "    # print(input_surface.shape)\n",
    "    input = Pad3D(input)\n",
    "    input_surface = Pad2D(input_surface)\n",
    "    # print(\"input shape after 3D padding: \", input.shape)\n",
    "    # print(\"input surface shape 2D after padding: \", input_surface.shape)\n",
    "    # Apply a linear projection for patch_size[0]*patch_size[1]*patch_size[2] patches, patch_size = (2, 4, 4) as in the original paper\n",
    "\n",
    "    # mod: reshape input from [14, 1440, 724, 5] to [5, 1440, 724, 14]\n",
    "    input = torch.permute(input, (3,0,2,1))\n",
    "\n",
    "    input = self.conv(input) # shape [192, 7, 181, 360]\n",
    "\n",
    "    # Add three constant fields to the surface fields\n",
    "    # input_surface = torch.concatenate((input_surface, self.land_mask, self.soil_type, self.topography))\n",
    "\n",
    "    # mod: reshape input\n",
    "    input_surface = torch.permute(input_surface, (2,1,0))\n",
    "    # Apply a linear projection for patch_size[1]*patch_size[2] patches\n",
    "    input_surface = self.conv_surface(input_surface) # shape: [192, 181, 360]\n",
    "\n",
    "\n",
    "    # mod: prepare for cat\n",
    "    input_surface = input_surface.unsqueeze(1)\n",
    "    # Concatenate the input in the pressure level, i.e., in Z dimension\n",
    "    x = torch.concat((input, input_surface), dim=1)\n",
    "    # temp mod: batch is not considered, thus unsqueeze a batch_size = 1, otherwise change the LoadData function to include batch dim\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    # Reshape x for calculation of linear projections\n",
    "    x = torch.permute(x, (0, 2, 3, 4, 1)) # [1, 8, 181, 360, 192] # C=192\n",
    "    x = reshape(x, shape=(x.shape[0], 8*360*181, x.shape[-1]))\n",
    "    print(\"dummy x successfully constructed with shape:\", x.shape)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oyNDEKSWtM8y"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchRecovery(nn.Module):\n",
    "  def __init__(self, patch_size, dim):\n",
    "    super().__init__()\n",
    "    '''Patch recovery operation'''\n",
    "    # Hear we use two transposed convolutions to recover data\n",
    "    self.conv = ConvTranspose3d(in_channels=dim, out_channels=5, kernel_size=patch_size, stride=patch_size)\n",
    "    self.conv_surface = ConvTranspose2d(in_channels=dim, out_channels=4, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "\n",
    "  def forward(self, x, Z, H, W):\n",
    "    # The inverse operation of the patch embedding operation, patch_size = (2, 4, 4) as in the original paper\n",
    "    # Reshape x back to three dimensions\n",
    "    x = permute(x, (0, 2, 1))\n",
    "    x = reshape(x, shape=(x.shape[0], x.shape[1], Z, H, W))\n",
    "\n",
    "    # Call the transposed convolution\n",
    "    output = self.conv(x[:, :, 1:, :, :])\n",
    "    output_surface = self.conv_surface(x[:, :, 0, :, :])\n",
    "\n",
    "    # Crop the output to remove zero-paddings\n",
    "    output = Crop3D(output)\n",
    "    output_surface = Crop2D(output_surface)\n",
    "    return output, output_surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vRJTgvZltRzl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DownSample(nn.Module):\n",
    "  def __init__(self, dim):\n",
    "    super().__init__()\n",
    "    '''Down-sampling operation'''\n",
    "    # A linear function and a layer normalization\n",
    "    self.linear = Linear(4*dim, 2*dim, bias=False)\n",
    "    self.norm = LayerNorm(4*dim)\n",
    "\n",
    "  def forward(self, x, Z, H, W):\n",
    "    # Reshape x to three dimensions for downsampling\n",
    "    x = reshape(x, shape=(x.shape[0], Z, H, W, x.shape[-1]))\n",
    "\n",
    "    # Padding the input to facilitate downsampling\n",
    "    x = Pad3D(x)\n",
    "\n",
    "    # Reorganize x to reduce the resolution: simply change the order and downsample from (8, 360, 182) to (8, 180, 91)\n",
    "    Z, H, W = x.shape\n",
    "    # Reshape x to facilitate downsampling\n",
    "    x = reshape(x, shape=(x.shape[0], Z, H//2, 2, W//2, 2, x.shape[-1]))\n",
    "    # Change the order of x\n",
    "    x = permute(x, (0,1,2,4,3,5,6))\n",
    "    # Reshape to get a tensor of resolution (8, 180, 91)\n",
    "    x = reshape(x, shape=(x.shape[0], Z*(H//2)*(W//2), 4 * x.shape[-1]))\n",
    "\n",
    "    # Call the layer normalization\n",
    "    x = self.norm(x)\n",
    "\n",
    "    # Decrease the channels of the data to reduce computation cost\n",
    "    x = self.linear(x)\n",
    "    return x\n",
    "\n",
    "class UpSample:\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    '''Up-sampling operation'''\n",
    "\n",
    "    # Linear layers without bias to increase channels of the data\n",
    "    self.linear1 = Linear(input_dim, output_dim*4, bias=False)\n",
    "\n",
    "    # Linear layers without bias to mix the data up\n",
    "    self.linear2 = Linear(output_dim, output_dim, bias=False)\n",
    "\n",
    "    # Normalization\n",
    "    self.norm = LayerNorm(output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Call the linear functions to increase channels of the data\n",
    "    x = self.linear1(x)\n",
    "\n",
    "    # Reorganize x to increase the resolution: simply change the order and upsample from (8, 180, 91) to (8, 360, 182)\n",
    "    # Reshape x to facilitate upsampling.\n",
    "    x = reshape(x, shape=(x.shape[0], 8, 180, 91, 2, 2, x.shape[-1]//4))\n",
    "    # Change the order of x\n",
    "    x = permute(x, (0,1,2,4,3,5,6))\n",
    "    # Reshape to get Tensor with a resolution of (8, 360, 182)\n",
    "    x = reshape(x, shape=(x.shape[0], 8, 360, 182, x.shape[-1]))\n",
    "\n",
    "    # Crop the output to the input shape of the network\n",
    "    x = Crop3D(x)\n",
    "\n",
    "    # Reshape x back\n",
    "    x = reshape(x, shape=(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[-1]))\n",
    "\n",
    "    # Call the layer normalization\n",
    "    x = self.norm(x)\n",
    "\n",
    "    # Mixup normalized tensors\n",
    "    x = self.linear2(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9lf03TyftVg-"
   },
   "outputs": [],
   "source": [
    "\n",
    "class EarthSpecificLayer(nn.Module):\n",
    "  def __init__(self, depth, dim, drop_path_ratio_list, heads):\n",
    "    '''Basic layer of our network, contains 2 or 6 blocks'''\n",
    "    super().__init__()\n",
    "    self.depth = depth\n",
    "    self.blocks = []\n",
    "\n",
    "    # Construct basic blocks\n",
    "    # print(dim, \"\\ndrop_path_ratio_list: \", drop_path_ratio_list.shape, \"\\nheads: \",heads)\n",
    "    for i in range(depth):\n",
    "      # print()\n",
    "      self.blocks.append(EarthSpecificBlock(dim, drop_path_ratio_list[i], heads))\n",
    "      # self.blocks.append(EarthSpecificBlock(dim, heads))\n",
    "\n",
    "  def forward(self, x, Z, H, W):\n",
    "    for i in range(self.depth):\n",
    "      # Roll the input every two blocks\n",
    "      if i % 2 == 0:\n",
    "        self.blocks[i](x, Z, H, W, roll=False)\n",
    "      else:\n",
    "        self.blocks[i](x, Z, H, W, roll=True)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CUMpivY8tY4v"
   },
   "outputs": [],
   "source": [
    "class EarthSpecificBlock(nn.Module):\n",
    "  def __init__(self, dim, drop_path_ratio, heads):\n",
    "    super().__init__()\n",
    "    '''\n",
    "    3D transformer block with Earth-Specific bias and window attention,\n",
    "    see https://github.com/microsoft/Swin-Transformer for the official implementation of 2D window attention.\n",
    "    The major difference is that we expand the dimensions to 3 and replace the relative position bias with Earth-Specific bias.\n",
    "    '''\n",
    "    # Define the window size of the neural network\n",
    "    self.window_size = (2, 6, 12)\n",
    "\n",
    "    # Initialize serveral operations\n",
    "    self.drop_path = DropPath(drop_prob=drop_path_ratio)\n",
    "    self.norm1 = LayerNorm(dim)\n",
    "    self.norm2 = LayerNorm(dim)\n",
    "    self.linear = MLP(dim, 0)\n",
    "    self.attention = EarthAttention3D(dim, heads, 0, self.window_size)\n",
    "\n",
    "  def forward(self, x, Z, H, W, roll):\n",
    "    # Save the shortcut for skip-connection\n",
    "    shortcut = x # [1, 521280, 192]\n",
    "    print(x.shape, self.window_size)\n",
    "    # Reshape input to three dimensions to calculate window attention\n",
    "    x = reshape(x, shape=(x.shape[0], Z, H, W, x.shape[2])) # [1, 8, 360, 181, 192]\n",
    "\n",
    "    # Zero-pad input if needed\n",
    "    print(x.shape)\n",
    "    x = Pad3D(x, self.window_size)\n",
    "    print(\"after padding\", x.shape)\n",
    "\n",
    "    # Store the shape of the input for restoration\n",
    "    ori_shape = x.shape\n",
    "\n",
    "    if roll:\n",
    "      # Roll x for half of the window for 3 dimensions\n",
    "      x = roll3D(x, shift=(self.window_size[0]//2, self.window_size[1]//2, self.window_size[2]//2))\n",
    "      # Generate mask of attention masks\n",
    "      # If two pixels are not adjacent, then mask the attention between them\n",
    "      # Your can set the matrix element to -1000 when it is not adjacent, then add it to the attention\n",
    "      mask = gen_mask(x)\n",
    "    else:\n",
    "      # e.g., zero matrix when you add mask to attention\n",
    "      mask = no_mask()\n",
    "    print(x.shape)\n",
    "    # Reorganize data to calculate window attention\n",
    "    # temp mod +1\n",
    "    x_window = reshape(x, shape=(x.shape[0], Z//self.window_size[0], self.window_size[0],\n",
    "                                 H // self.window_size[1], self.window_size[1],\n",
    "                                 W // self.window_size[2] + 1, self.window_size[2],\n",
    "                                 x.shape[-1])) # [1, 8, 360, 192, 192] -> [1, 4, 60, 16, 2, 6, 12, 192]\n",
    "    x_window = permute(x_window, (0, 1, 3, 5, 2, 4, 6, 7))\n",
    "    print(\"x_window: \", x_window.shape)\n",
    "\n",
    "    # Get data stacked in 3D cubes, which will further be used to calculated attention among each cube\n",
    "    x_window = reshape(x_window, shape=(-1, self.window_size[0]* self.window_size[1]*self.window_size[2], x.shape[-1]))\n",
    "    print(\"x_window: \", x_window.shape)\n",
    "    # Apply 3D window attention with Earth-Specific bias\n",
    "    x_window = self.attention(x, mask)\n",
    "\n",
    "    # Reorganize data to original shapes\n",
    "    # mod: W to (W+1) since padding\n",
    "    x = reshape(x_window, shape=((-1, Z // self.window_size[0],\n",
    "                                  H // self.window_size[1],\n",
    "                                   (W // self.window_size[2]) + 1,\n",
    "                                  self.window_size[0],\n",
    "                                  self.window_size[1],\n",
    "                                  self.window_size[2],\n",
    "                                  x_window.shape[-1])))\n",
    "    # x = reshape(x_window, shape=((-1, Z // self.window_size[0],\n",
    "    #                               H // self.window_size[1],\n",
    "    #                               W // self.window_size[2],\n",
    "    #                               self.window_size[0],\n",
    "    #                               self.window_size[1],\n",
    "    #                               self.window_size[2],\n",
    "    #                               x_window.shape[-1])))\n",
    "    print(\"hihihi\", x.shape)\n",
    "    x = permute(x, (0, 1, 4, 2, 5, 3, 6, 7))\n",
    "\n",
    "    # Reshape the tensor back to its original shape\n",
    "    x = reshape(x_window, shape=ori_shape)\n",
    "\n",
    "    if roll:\n",
    "      # Roll x back for half of the window\n",
    "      x = roll3D(x, shift=[-self.window_size[0]//2, -self.window_size[1]//2, -self.window_size[2]//2])\n",
    "\n",
    "    # Crop the zero-padding\n",
    "    x = Crop3D(x)\n",
    "\n",
    "    # Reshape the tensor back to the input shape\n",
    "    x = reshape(x, shape=(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[4]))\n",
    "\n",
    "    # Main calculation stages\n",
    "    x = shortcut + self.drop_path(self.norm1(x))\n",
    "    x = x + self.drop_path(self.norm2(self.linear(x)))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O3i-9G5BtZJF"
   },
   "outputs": [],
   "source": [
    "class EarthAttention3D(nn.Module): # TODO\n",
    "  def __init__(self, dim, heads, dropout_rate, window_size):\n",
    "    super().__init__()\n",
    "    '''\n",
    "    3D window attention with the Earth-Specific bias,\n",
    "    see https://github.com/microsoft/Swin-Transformer for the official implementation of 2D window attention.\n",
    "    '''\n",
    "    # Initialize several operations\n",
    "    self.linear1 = Linear(dim, 3, bias=True)\n",
    "    self.linear2 = Linear(dim, dim)\n",
    "    self.softmax = Softmax(dim=-1)\n",
    "    self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    # Store several attributes\n",
    "    self.head_number = heads\n",
    "    self.dim = dim\n",
    "    self.scale = (dim//heads)**-0.5\n",
    "    self.window_size = window_size\n",
    "\n",
    "    # input_shape is current shape of the self.forward function\n",
    "    # You can run your code to record it, modify the code and rerun it\n",
    "    # Record the number of different window types\n",
    "    # ?? not sure\n",
    "    self.type_of_windows = 10\n",
    "    # self.type_of_windows = (input_shape[0]//window_size[0])*(input_shape[1]//window_size[1])\n",
    "\n",
    "    # For each type of window, we will construct a set of parameters according to the paper\n",
    "    self.earth_specific_bias = ConstructTensor(size=((2 * window_size[2] - 1) *\n",
    "                                                     window_size[1] * window_size[1] *\n",
    "                                                     window_size[0] * window_size[0],\n",
    "                                                     self.type_of_windows, heads))\n",
    "    # print(\"self.earth_specific_bias\", self.earth_specific_bias)\n",
    "    # self.earth_specific_bias = ConstructTensor(size=((window_size[2]**2) * window_size[1] * window_size[1] * window_size[0] * window_size[0], self.type_of_windows, heads))\n",
    "\n",
    "    # Making these tensors to be learnable parameters\n",
    "    self.earth_specific_bias = nn.Parameter(self.earth_specific_bias)\n",
    "\n",
    "    # Initialize the tensors using Truncated normal distribution\n",
    "    TruncatedNormalInit(self.earth_specific_bias, std=0.02)\n",
    "\n",
    "    # Construct position index to reuse self.earth_specific_bias\n",
    "    self._construct_index()\n",
    "    # print(\"Construct position index\", self.position_index)\n",
    "\n",
    "  def _construct_index(self):\n",
    "    ''' This function construct the position index to reuse symmetrical parameters of the position bias'''\n",
    "    # Index in the pressure level of query matrix\n",
    "    coords_zi = torch.arange(self.window_size[0])\n",
    "    # Index in the pressure level of key matrix\n",
    "    coords_zj = -torch.arange(self.window_size[0])*self.window_size[0]\n",
    "\n",
    "    # Index in the latitude of query matrix\n",
    "    coords_hi = torch.arange(self.window_size[1])\n",
    "    # Index in the latitude of key matrix\n",
    "    coords_hj = -torch.arange(self.window_size[1])*self.window_size[1]\n",
    "\n",
    "    # Index in the longitude of the key-value pair\n",
    "    coords_w = torch.arange(self.window_size[2])\n",
    "\n",
    "    # Change the order of the index to calculate the index in total\n",
    "    coords_1 = stack(torch.meshgrid([coords_zi, coords_hi, coords_w]))\n",
    "    coords_2 = stack(torch.meshgrid([coords_zj, coords_hj, coords_w]))\n",
    "    coords_flatten_1 = flatten(coords_1, start_dim=1)\n",
    "    coords_flatten_2 = flatten(coords_2, start_dim=1)\n",
    "    coords = coords_flatten_1[:, :, None] - coords_flatten_2[:, None, :]\n",
    "    coords = permute(coords, (1, 2, 0))\n",
    "\n",
    "    # Shift the index for each dimension to start from 0\n",
    "    coords[:, :, 2] += self.window_size[2] - 1\n",
    "    coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "    coords[:, :, 0] *= (2 * self.window_size[2] - 1)*self.window_size[1]*self.window_size[1]\n",
    "    # print(\"~~~~~~~~~~~~~~~\", coords)\n",
    "    # Sum up the indexes in three dimensions\n",
    "    self.position_index = torch.sum(coords, dim=-1)\n",
    "    # print(\" self.position_index\",  self.position_index)\n",
    "    # Flatten the position index to facilitate further indexing\n",
    "    self.position_index = flatten(self.position_index)\n",
    "    print(\" !!!!self.position_index shape\",  self.position_index.shape)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # Linear layer to create query, key and value\n",
    "    print(\"before linear\", x.shape)\n",
    "    x = self.linear1(x)\n",
    "    print(\"after linear\", x.shape)\n",
    "    # Record the original shape of the input\n",
    "    original_shape = x.shape\n",
    "    print(\"original_shape: \", x.shape) # [1, 8, 360, 192, 3]\n",
    "    # reshape the data to calculate multi-head attention\n",
    "\n",
    "    # mod: TODO: ask why shape does not match with dim\n",
    "    # qkv = reshape(x, shape=(x.shape[0],\n",
    "    #                         x.shape[1],\n",
    "    #                         3,\n",
    "    #                         self.head_number,\n",
    "    #                         self.dim // self.head_number))\n",
    "    print(\"Warning!!!!!! self.dim = 69120 to pass run!\")\n",
    "    qkv = reshape(x, shape=(x.shape[0],\n",
    "                            x.shape[1],\n",
    "                            3,\n",
    "                            self.head_number,\n",
    "                            69120 // self.head_number))\n",
    "    query, key, value = permute(qkv, (2, 0, 3, 1, 4)) # permute: [3, 1,head_num=6,8,11520]\n",
    "\n",
    "    # Scale the attention\n",
    "    query = query * self.scale # [1, 6, 8, 11520]\n",
    "    print(query.shape)\n",
    "    # Calculated the attention, a learnable bias is added to fix the nonuniformity of the grid.\n",
    "    attention = query @ key.transpose(2,3) # @ denotes matrix multiplication\n",
    "    print(\"attention shape\", attention.shape) # [1, 6, 8, 8]\n",
    "    # self.earth_specific_bias is a set of neural network parameters to optimize.\n",
    "    EarthSpecificBias = self.earth_specific_bias[self.position_index] #[20736, 10, 6]\n",
    "    print(\"self.position_index\", self.position_index)\n",
    "    print(\"EarthSpecificBias shape\", EarthSpecificBias.shape) #[20736, 10, 6]\n",
    "\n",
    "    # Reshape the learnable bias to the same shape as the attention matrix\n",
    "    EarthSpecificBias = reshape(EarthSpecificBias, shape=(self.window_size[0]*self.window_size[1]*self.window_size[2],\n",
    "                                                          self.window_size[0]*self.window_size[1]*self.window_size[2],\n",
    "                                                          self.type_of_windows,\n",
    "                                                          self.head_number))\n",
    "    EarthSpecificBias = permute(EarthSpecificBias, (2, 3, 0, 1))\n",
    "    # EarthSpecificBias = reshape(EarthSpecificBias, shape = [1]+EarthSpecificBias.shape)\n",
    "    EarthSpecificBias = torch.unsqueeze(EarthSpecificBias, dim=0)\n",
    "    # Add the Earth-Specific bias to the attention matrix\n",
    "    print(attention.shape, EarthSpecificBias.shape)\n",
    "    attention = attention + EarthSpecificBias\n",
    "\n",
    "    # Mask the attention between non-adjacent pixels, e.g., simply add -100 to the masked element.\n",
    "    attention = self.mask_attention(attention, mask)\n",
    "    attention = self.softmax(attention)\n",
    "    attention = self.dropout(attention)\n",
    "\n",
    "    # Calculated the tensor after spatial mixing.\n",
    "    x = attention @ value.T # @ denote matrix multiplication\n",
    "\n",
    "    # Reshape tensor to the original shape\n",
    "    x = permute(x, (0, 2, 1))\n",
    "    x = reshape(x, shape = original_shape)\n",
    "\n",
    "    # Linear layer to post-process operated tensor\n",
    "    x = self.linear2(x)\n",
    "    x = self.dropout(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "irZt6Qjctc4M"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, dim, dropout_rate):\n",
    "    super().__init__()\n",
    "    '''MLP layers, same as most vision transformer architectures.'''\n",
    "    self.linear1 = Linear(dim, dim * 4)\n",
    "    self.linear2 = Linear(dim * 4, dim)\n",
    "    self.activation = GELU()\n",
    "    self.drop = Dropout(dropout_rate)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.linear(x)\n",
    "    x = self.drop(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iTXzOSV7LPEl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def Inference(input, input_surface, forecast_range):\n",
    "  '''Inference code, describing the algorithm of inference using models with different lead times.\n",
    "  PanguModel24, PanguModel6, PanguModel3 and PanguModel1 share the same training algorithm but differ in lead times.\n",
    "  Args:\n",
    "    input: input tensor, need to be normalized to N(0, 1) in practice\n",
    "    input_surface: target tensor, need to be normalized to N(0, 1) in practice\n",
    "    forecast_range: iteration numbers when roll out the forecast model\n",
    "  '''\n",
    "\n",
    "  PanguModel24 = LoadModel(ModelPath24)\n",
    "  PanguModel6 = LoadModel(ModelPath6)\n",
    "  PanguModel3 = LoadModel(ModelPath3)\n",
    "  PanguModel1 = LoadModel(ModelPath1)\n",
    "\n",
    "  weather_mean, weather_std, weather_surface_mean, weather_surface_std = LoadStatic()\n",
    "\n",
    "  input_24, input_surface_24 = input, input_surface\n",
    "  input_6, input_surface_6 = input, input_surface\n",
    "  input_3, input_surface_3 = input, input_surface\n",
    "\n",
    "  output_list = []\n",
    "\n",
    "  # Note: the following code is implemented for fast inference of [1,forecast_range]-hour forecasts -- if only one lead time is requested, the inference can be much faster.\n",
    "  for i in range(forecast_range):\n",
    "    # switch to the 24-hour model if the forecast time is 24 hours, 48 hours, ..., 24*N hours\n",
    "    if (i+1) % 24 == 0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_24, input_surface_24\n",
    "\n",
    "      # Call the model pretrained for 24 hours forecast\n",
    "      output, output_surface = PanguModel24(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "      # Stored the output for next round forecast\n",
    "      input_24, input_surface_24 = output, output_surface\n",
    "      input_6, input_surface_6 = output, output_surface\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 6-hour model if the forecast time is 30 hours, 36 hours, ..., 24*N + 6/12/18 hours\n",
    "    elif (i+1) % 6 == 0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_6, input_surface_6\n",
    "\n",
    "      # Call the model pretrained for 6 hours forecast\n",
    "      output, output_surface = PanguModel6(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "      # Stored the output for next round forecast\n",
    "      input_6, input_surface_6 = output, output_surface\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 3-hour model if the forecast time is 3 hours, 9 hours, ..., 6*N + 3 hours\n",
    "    elif (i+1) % 3 ==0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_3, input_surface_3\n",
    "\n",
    "      # Call the model pretrained for 3 hours forecast\n",
    "      output, output_surface = PanguModel3(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "      # Stored the output for next round forecast\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 1-hour model\n",
    "    else:\n",
    "      # Call the model pretrained for 1 hours forecast\n",
    "      output, output_surface = PanguModel1(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "    # Stored the output for next round forecast\n",
    "    input, input_surface = output, output_surface\n",
    "\n",
    "    # Save the output\n",
    "    output_list.append((output, output_surface))\n",
    "  return output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Z_ODpzJms_J7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    model = PanguModel()\n",
    "    optimizer = Adam(model.parameters(), lr=5e-4, weight_decay=3e-6)\n",
    "    epochs = 100\n",
    "    for i in range(epochs):  # Loop from 1979 to 2017\n",
    "        dataset_length = 5\n",
    "        for step in range(dataset_length):\n",
    "\n",
    "            inputs, inputs_surface, targets, targets_surface = LoadData(step)\n",
    "            # inputs, inputs_surface, targets, targets_surface = inputs.to(device), inputs_surface.to(device), targets.to(device), targets_surface.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, outputs_surface = model(inputs, inputs_surface)\n",
    "\n",
    "            # Calculate loss: MAE loss for both output and surface output, with additional weight for the surface loss\n",
    "            # loss = TensorAbs(output-target) + TensorAbs(output_surface-target_surface) * 0.25\n",
    "            loss = F.l1_loss(outputs, targets) + 0.25 * F.l1_loss(outputs_surface, targets_surface)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"what???\")\n",
    "\n",
    "        print(f'Epoch [{i+1}/{epochs}], Loss: {total_loss/len(dataset_length)}')\n",
    "    torch.save(model.state_dict(), 'pangu_weather_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOWgXuIFt2Wy"
   },
   "source": [
    "## here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Wvi35rSuIHgJ"
   },
   "outputs": [],
   "source": [
    "inputs, inputs_surface, targets, targets_surface = LoadData(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VHckRXnGR086"
   },
   "outputs": [],
   "source": [
    "# LoadConstantMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3LjkOGWuhHM",
    "outputId": "6db70f6d-50d2-44c5-d01b-40b36757a1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude_padding 3\n"
     ]
    }
   ],
   "source": [
    "## here\n",
    "\n",
    "patch_size = (2,4,4)\n",
    "conv = Conv3d(in_channels=5, out_channels=192, kernel_size=(2,4,4), stride=(2,4,4))\n",
    "conv_surface = Conv2d(in_channels=4, out_channels=192, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "\n",
    "input_pad = Pad3D(inputs)\n",
    "input_pad = torch.permute(input_pad, (3,0,2,1))\n",
    "input_cov = conv(input_pad)\n",
    "\n",
    "inputs_surface_pad = Pad2D(inputs_surface)\n",
    "inputs_surface_pad = torch.permute(inputs_surface_pad, (2,1,0))\n",
    "input_surface_conv = conv_surface(inputs_surface_pad)\n",
    "input_surface_conv = input_surface_conv.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bLkFMCFRv9T",
    "outputId": "110755c8-4bfd-415e-ad20-c7e553764fb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 7, 181, 360])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Pi8_AeCwZNBW"
   },
   "outputs": [],
   "source": [
    "# input_surface_conv = input_surface_conv.unsqueeze(0)\n",
    "# input_surface_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsdbNjaqZKKg",
    "outputId": "edb0a2db-7a60-431d-b01f-c9be6960131b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 8, 181, 360])\n",
      "torch.Size([1, 192, 8, 181, 360])\n",
      "torch.Size([1, 8, 181, 360, 192])\n"
     ]
    }
   ],
   "source": [
    "x = torch.concat((input_cov, input_surface_conv), dim=1)\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(0)\n",
    "print(x.shape)\n",
    "x = torch.permute(x, (0, 2, 3, 4, 1))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YuHse6qQZrbf",
    "outputId": "3d1397bc-98e3-463c-f263-2c374207b810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "181 // 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KxHmH282uTdg",
    "outputId": "fcf76dc3-31d2-42ce-e306-3d2ae18ed6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.earth_specific_bias tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "self.earth_specific_bias tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]]])\n",
      " !!!!self.position_index shape torch.Size([20736])\n",
      "latitude_padding 3\n",
      "dummy x successfully constructed with shape: torch.Size([1, 521280, 192])\n",
      "torch.Size([1, 521280, 192]) (2, 6, 12)\n",
      "torch.Size([1, 8, 360, 181, 192])\n",
      "latitude_padding 11\n",
      "after padding torch.Size([1, 8, 360, 192, 192])\n",
      "torch.Size([1, 8, 360, 192, 192])\n",
      "x_window:  torch.Size([1, 4, 60, 16, 2, 6, 12, 192])\n",
      "x_window:  torch.Size([3840, 144, 192])\n",
      "before linear torch.Size([1, 8, 360, 192, 192])\n",
      "after linear torch.Size([1, 8, 360, 192, 3])\n",
      "original_shape:  torch.Size([1, 8, 360, 192, 3])\n",
      "Warning!!!!!! self.dim = 69120 to pass run!\n",
      "torch.Size([1, 6, 8, 11520])\n",
      "attention shape torch.Size([1, 6, 8, 8])\n",
      "self.position_index tensor([  11,   10,    9,  ..., 3302, 3301, 3300])\n",
      "EarthSpecificBias shape torch.Size([20736, 10, 6])\n",
      "torch.Size([1, 6, 8, 8]) torch.Size([1, 10, 6, 144, 144])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (144) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2da0ffaf5447>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-7c508990f6eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_surface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_surface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Calculate loss: MAE loss for both output and surface output, with additional weight for the surface loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c0dead0a4c21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, input_surface)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Encoder, composed of two layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Layer 1, shape (8, 360, 181, C), C = 192 as in the original paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m360\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m181\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Store the tensor for skip-connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-75b72bc60959>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, Z, H, W)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m# Roll the input every two blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-2744207eb05d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, Z, H, W, roll)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x_window: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Apply 3D window attention with Earth-Specific bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mx_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Reorganize data to original shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d8ad2af50e89>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# Add the Earth-Specific bias to the attention matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarthSpecificBias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mEarthSpecificBias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# Mask the attention between non-adjacent pixels, e.g., simply add -100 to the masked element.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (144) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHLjutdzjYHf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-TJFURWjYHf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
